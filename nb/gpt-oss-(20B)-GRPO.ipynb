{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShlokVFX/deepseek-r1-clone/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQzu1kfb7oT"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bV9PBL5b7oW"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRvyp0Tpb7oX"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05fvRDRlb7oY"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYBwh6k5b7oZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, importlib.util\n",
        "!pip install --upgrade -qqq uv\n",
        "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    !uv pip install -qqq \\\n",
        "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
        "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "        git+https://github.com/triton-lang/triton.git@0add68262ab0a2e33b84524346cb27cbb2787356#subdirectory=python/triton_kernels\n",
        "elif importlib.util.find_spec(\"unsloth\") is None:\n",
        "    !uv pip install -qqq unsloth\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkH_y8UC9lvv"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPgFeIkZn9q"
      },
      "source": [
        "# Goal: Make faster kernels with Reinforcement Learning\n",
        "\n",
        "Our goal is to make a faster matrix multiplication kernel by doing RL on GTP-OSS 20B with Unsloth.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Matrix_multiplication_qtl1.svg/500px-Matrix_multiplication_qtl1.svg.png\" height=200 />\n",
        "\n",
        "You will learn how to:\n",
        "1. Counteract **reward hacking** like cheating, caching, laziness.\n",
        "2. Timing and correctness of kernels and time limits.\n",
        "3. Making good **reward functions**\n",
        "4. How to seriously do RL to make optimized CUDA kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 768 # Can increase for longer RL output\n",
        "lora_rank = 4 # Larger rank = smarter, but slower\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gpt-oss-20b\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    offload_embedding = True, # Reduces VRAM by 1GB\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfeUs-lQJDSq"
      },
      "source": [
        "We now add some small amount of LoRA weights to GPT-OSS so we only need to train those, instead of training on the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8rGa-o3HJCo1"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0QnO9_YJBOI"
      },
      "source": [
        "# Optimized matrix multiplication\n",
        "\n",
        "Numpy has optimized matrix multiplication kernels for CPUs via BLAS optimized operations. For GPUs, one can use CUDA accelerated cuBLAS kernels which PyTorch calls under the hood.\n",
        "\n",
        "To generate some random matrices to do matrix multiplication, we can do the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D9CI4jtgL5mw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def generate_random_matrices(seed = 3407, n = 256):\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    n, k, m = random_state.randint(1, n+1, size = 3)\n",
        "    A = np.random.uniform(-10, 10, size = (n, k))\n",
        "    B = np.random.uniform(-10, 10, size = (k, m))\n",
        "    return A, A.tolist(), B, B.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BcaLniVKLpa"
      },
      "source": [
        "We shall generate a small matrix, and see the matrix multiplied output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-M8kGaFRJ2ic"
      },
      "outputs": [],
      "source": [
        "A, A_list, B, B_list = generate_random_matrices(seed = 42, n = 5)\n",
        "print(A)\n",
        "print(B)\n",
        "print(np.matmul(A, B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envzrXmjKRff"
      },
      "source": [
        "We can call a LLM to generate a simple matrix multiply kernel in Python only, and we can calculate the differences between the actual result and the kernel's result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b-gSgthFI_wq"
      },
      "outputs": [],
      "source": [
        "def calculate_difference(pred, real):\n",
        "    if pred is None: return 5, 5\n",
        "    assert real is not None\n",
        "    import numpy as np\n",
        "    try:\n",
        "        difference = pred - real\n",
        "    except:\n",
        "        return 5, 5\n",
        "    amax_error = float(np.amax(difference))\n",
        "    mse_error  = float(np.mean(np.square(difference)))\n",
        "    return amax_error, mse_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q9gmkbTnKbcF"
      },
      "outputs": [],
      "source": [
        "# Kernel generated by GPT-5\n",
        "def matmul(A, B):\n",
        "    z, s = zip, sum\n",
        "    Bt = list(z(*B))\n",
        "    return [[s(a*b for a, b in z(row, col)) for col in Bt] for row in A]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-WfRwQeKtEZ"
      },
      "source": [
        "We see the error below is very small, so that's good!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_QvIidsPKg2C"
      },
      "outputs": [],
      "source": [
        "prediction = matmul(A_list, B_list)\n",
        "calculate_difference(prediction, np.matmul(A, B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR6czU96cpxf"
      },
      "source": [
        "# Countering Reward Hacking\n",
        "\n",
        "The ultimate goal of RL is to maximize some reward (say speed, revenue, some metric).\n",
        "\n",
        "But RL can **cheat** When the RL algorithm learns a trick or exploits something to increase the reward, without actually doing the task at end, this is called \"Reward Hacking\".\n",
        "\n",
        "Some good examples are in https://en.wikipedia.org/wiki/Reward_hacking\n",
        "\n",
        "For matrix multiplication kernels, we might see the following issues:\n",
        "\n",
        "* Laziness: RL learns to use Numpy, Torch, other libraries, which calls optimized CUDA kernels.\n",
        "* Caching: RL learns to cache the result of the output\n",
        "* Cheating: RL learns to find the actual output by inspecting Python global variables\n",
        "* RL learns to edit the timing function to make it output 0 time as passed.\n",
        "\n",
        "And possibly more. We shall try to address each!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRhLV_bZMYxy"
      },
      "source": [
        "# Countering Reward Hacking 1: Stop laziness\n",
        "We can stop the RL algorithm from calling optimized code by inspecting if the generated code imports other non standard Python libraries. We used GPT-5 to help generate this check `check_only_stdlib_imports`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "MXPZ1MsUMqNZ"
      },
      "outputs": [],
      "source": [
        "#@title (Collapsible code)\n",
        "import ast\n",
        "import sys\n",
        "import sysconfig\n",
        "from pathlib import Path\n",
        "\n",
        "def _stdlib_names():\n",
        "    \"\"\"\n",
        "    Build a set of canonical stdlib top-level module/package names.\n",
        "    Uses sys.stdlib_module_names when available (3.10+), with a\n",
        "    filesystem fallback for older versions/edge cases.\n",
        "    \"\"\"\n",
        "    names = {m.lower() for m in getattr(sys, \"stdlib_module_names\", set())}\n",
        "    names |= {m.lower() for m in sys.builtin_module_names}\n",
        "    names.add(\"__future__\")  # special-case\n",
        "\n",
        "    # Fallback/augmentation: scan the stdlib directory\n",
        "    try:\n",
        "        stdlib_dir = Path(sysconfig.get_path(\"stdlib\"))\n",
        "        if stdlib_dir.exists():\n",
        "            for p in stdlib_dir.iterdir():\n",
        "                if p.name == \"site-packages\":\n",
        "                    continue\n",
        "                if p.suffix == \".py\":\n",
        "                    names.add(p.stem.lower())\n",
        "                elif p.is_dir() and (p / \"__init__.py\").exists():\n",
        "                    names.add(p.name.lower())\n",
        "    except Exception:\n",
        "        # conservative fallback; the names set above will still work well\n",
        "        pass\n",
        "\n",
        "    return names\n",
        "\n",
        "_STDLIB_SET = _stdlib_names()\n",
        "\n",
        "def check_only_stdlib_imports(code: str):\n",
        "    \"\"\"\n",
        "    Return (ok: bool, details: dict)\n",
        "\n",
        "    ok == True  -> all absolute imports are from the stdlib.\n",
        "    ok == False -> details['non_stdlib'] lists offending top-level modules.\n",
        "\n",
        "    details includes:\n",
        "      - stdlib: sorted list of stdlib imports found\n",
        "      - non_stdlib: sorted list of non-stdlib imports found\n",
        "      - relative_imports: count of relative imports (always allowed here)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "    except SyntaxError as e:\n",
        "        return False, {\n",
        "            \"error\": f\"SyntaxError: {e}\",\n",
        "            \"stdlib\": [],\n",
        "            \"non_stdlib\": [],\n",
        "            \"relative_imports\": 0,\n",
        "        }\n",
        "\n",
        "    abs_imports = set()\n",
        "    relative_count = 0\n",
        "\n",
        "    class Visitor(ast.NodeVisitor):\n",
        "        def visit_Import(self, node: ast.Import):\n",
        "            for alias in node.names:\n",
        "                abs_imports.add(alias.name.split(\".\")[0])\n",
        "        def visit_ImportFrom(self, node: ast.ImportFrom):\n",
        "            nonlocal relative_count\n",
        "            if (node.level or 0) > 0:\n",
        "                # relative import\n",
        "                relative_count += 1\n",
        "            else:\n",
        "                if node.module:\n",
        "                    abs_imports.add(node.module.split(\".\")[0])\n",
        "\n",
        "    Visitor().visit(tree)\n",
        "\n",
        "    stdlib_found = sorted(m for m in abs_imports if m.lower() in _STDLIB_SET)\n",
        "    non_stdlib = sorted(m for m in abs_imports if m.lower() not in _STDLIB_SET)\n",
        "\n",
        "    return len(non_stdlib) == 0, {\n",
        "        \"stdlib\": stdlib_found,\n",
        "        \"non_stdlib\": non_stdlib,\n",
        "        \"relative_imports\": relative_count,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngUAw1lMM9JQ"
      },
      "source": [
        "For example, let's call `check_only_stdlib_imports` on a random piece of matrix multiplication code generated by GPT-5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zz80kvg6M4BG"
      },
      "outputs": [],
      "source": [
        "sample = \"\"\"\n",
        "def matmul(A, B):\n",
        "    import numpy as np\n",
        "    from torch import matmul\n",
        "    z, s = zip, sum\n",
        "    Bt = list(z(*B))\n",
        "    return [[s(a*b for a, b in z(row, col)) for col in Bt] for row in A]\n",
        "\"\"\"\n",
        "ok, info = check_only_stdlib_imports(sample)\n",
        "print(\"Only stdlib imports?\", ok)\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6lgkGkEN7B0"
      },
      "source": [
        "# Countering Reward Hacking 2: Stop cheating\n",
        "We can stop the RL algorithm from using global or cached variables by restricting it's `locals` and `globals`.\n",
        "\n",
        "We are also going to use `exec` to create the function, so we have to save the output to an empty dict.\n",
        "\n",
        "We also disallow global variable access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jrIeYu-lOLSv"
      },
      "outputs": [],
      "source": [
        "output_function = {}\n",
        "exec(sample, {}, output_function)\n",
        "output_function[\"matmul\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDSrjOTLVyQm"
      },
      "source": [
        "We also disallow global variable access via `types.FunctionType(f.__code__, {})`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GcmYAmohVqw2"
      },
      "outputs": [],
      "source": [
        "import types\n",
        "output_function[\"matmul\"] = types.FunctionType(output_function[\"matmul\"].__code__, {})\n",
        "\n",
        "def import_numpy():\n",
        "    np.matmul\n",
        "    print(\"Success\")\n",
        "\n",
        "import_numpy()\n",
        "import_numpy = types.FunctionType(import_numpy.__code__, {})\n",
        "try:\n",
        "    import_numpy()\n",
        "except Exception as e:\n",
        "    print(str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5tJKwLUgZsRq"
      },
      "outputs": [],
      "source": [
        "def create_locked_down_function(function):\n",
        "    output_function = {}\n",
        "    exec(function, {}, output_function)\n",
        "    new_matmul = output_function[\"matmul\"]\n",
        "    new_matmul = types.FunctionType(new_matmul.__code__, {})\n",
        "    return new_matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl-IxTZ6Nvm9"
      },
      "source": [
        "# Countering Reward Hacking 3: Stop caching\n",
        "We can stop the RL algorithm from using cached data by wiping the cache with a large fake matrix. We also have to benchmark carefully with multiple loops and turns.\n",
        "\n",
        "We also add a **timer** to not make the algorithm go in an endless loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jZwGdNyMNlEV"
      },
      "outputs": [],
      "source": [
        "import os, gc, time, statistics\n",
        "import signal\n",
        "from contextlib import contextmanager\n",
        "class TimeoutError(Exception): pass\n",
        "\n",
        "@contextmanager\n",
        "def time_limit(seconds):\n",
        "    def _handler(signum, frame):\n",
        "        raise TimeoutError(f\"Timed out after {seconds}s\")\n",
        "    old = signal.signal(signal.SIGALRM, _handler)\n",
        "    signal.setitimer(signal.ITIMER_REAL, seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.setitimer(signal.ITIMER_REAL, 0.0)\n",
        "        signal.signal(signal.SIGALRM, old)\n",
        "\n",
        "class Benchmarker:\n",
        "    def __init__(self, trials = 3, loops = 1, timeout = 30):\n",
        "        self.buffer = np.zeros(2 * 1024 * 1024 * 1024, dtype = np.uint8)\n",
        "        self.trials = trials\n",
        "        self.loops = loops\n",
        "        assert timeout > 0 # Cannot be 0 since it won't work!\n",
        "        self.timeout = timeout\n",
        "    def thrash(self):\n",
        "        # Edit the buffer to wipe cache lines\n",
        "        self.buffer ^= 1\n",
        "        return int(self.buffer[::4096].sum())\n",
        "\n",
        "    def benchmark(self, function, arguments):\n",
        "        assert len(arguments) == self.loops\n",
        "        samples = []\n",
        "        exceptions = []\n",
        "        timed_out = 0\n",
        "        for _ in range(self.trials):\n",
        "            gc.collect(); gc.disable(); self.thrash()\n",
        "            t_start = time.perf_counter_ns()\n",
        "            for i in range(self.loops):\n",
        "                try:\n",
        "                    with time_limit(self.timeout):\n",
        "                        function(*arguments[i])\n",
        "                except TimeoutError as e:\n",
        "                    timed_out += 1\n",
        "                except Exception as e:\n",
        "                    exceptions.append(str(e))\n",
        "            t_end = time.perf_counter_ns()\n",
        "            gc.enable()\n",
        "            samples.append((t_end - t_start) // max(1, self.loops))\n",
        "        return {\n",
        "            \"median_ns\": int(statistics.median(samples)),\n",
        "            \"mean_ns\": int(statistics.fmean(samples)),\n",
        "            \"stdev_ns\": int(statistics.pstdev(samples) if len(samples) > 1 else 0),\n",
        "            \"exceptions\" : exceptions,\n",
        "            \"timeouts\" : timed_out,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV5M0DCyOvon"
      },
      "source": [
        "For example we use our matmul kernel we had, and benchmark it with a 10 second delay:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8df8tZcEOuYJ"
      },
      "outputs": [],
      "source": [
        "A, A_list, B, B_list = generate_random_matrices(seed = 0, n = 256)\n",
        "Benchmarker(trials = 1, timeout = 10).benchmark(output_function[\"matmul\"], [(A_list, B_list)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CzwCyXIPK04"
      },
      "source": [
        "# Data & RL task setup\n",
        "\n",
        "We now have to create a prompt to the model for which it will do some task. For our matrix multiply example, we use the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B-2RRE4HMrQO"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Create a new fast matrix multiplication function using only native Python code.\n",
        "You are given a list of list of numbers.\n",
        "Output your new function in backticks using the format below:\n",
        "```python\n",
        "def matmul(A, B):\n",
        "    return ...\n",
        "```\n",
        "\"\"\".strip()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIdudFUodN4i"
      },
      "source": [
        "First, let's prompt GPT-OSS without RL and see how it goes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9HJxrS76h3Ds"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": prompt}],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    reasoning_effort = \"low\",\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 1.0,\n",
        "    max_new_tokens = 512,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iknaWZNudTNq"
      },
      "source": [
        "# Reward functions\n",
        "\n",
        "We now design the `extract_function` function which simply extracts the function wrapped in 3 backticks.\n",
        "\n",
        "And 4 reward functions:\n",
        "\n",
        "1. `function_works` which rewards the model if the strategy is a valid Python function.\n",
        "2. `no_cheating` which checks if the function imported other modules, and if it did, we penalize it.\n",
        "3. `correctness_check` which checks if the kernel was correct or wrong - it shouldn't generate gibberish!\n",
        "4. `speed_check` checks the performance relative to Numpy matmul directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8JJGXKdJ-Zl_"
      },
      "outputs": [],
      "source": [
        "def extract_function(text):\n",
        "    if text.count(\"```\") >= 2:\n",
        "        first = text.find(\"```\") + 3\n",
        "        second = text.find(\"```\", first)\n",
        "        fx = text[first : second].strip()\n",
        "        fx = fx.removeprefix(\"python\\n\")\n",
        "        fx = fx[fx.find(\"def\"):]\n",
        "        if fx.startswith(\"def matmul(A, B):\"): return fx\n",
        "    return None\n",
        "print(extract_function(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLXEcf_HSJlI"
      },
      "source": [
        "Below is our `function_works` reward function which uses Python's `exec` but guarded by not allowing leakage of local and global variables. We can also use `check_only_stdlib_imports` first to check if there are errors before even executing the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h3-B0IIsS56S"
      },
      "outputs": [],
      "source": [
        "ok, info = check_only_stdlib_imports(\"def a\")\n",
        "ok, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def function_works(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        print(function)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        if function is None or \"error\" in info:\n",
        "            score = -2.0\n",
        "        else:\n",
        "            try:\n",
        "                new_matmul = create_locked_down_function(function)\n",
        "                score = 1.0\n",
        "            except:\n",
        "                score = -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "`no_cheating` checks if the function cheated since it might have imported Numpy or Torch optimized code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def no_cheating(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        else:\n",
        "            ok = False\n",
        "        scores.append(1.0 if ok else -20.0) # Penalize heavily!\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slnqWG3FTror"
      },
      "source": [
        "Next `correctness_check` checks if the kernel was correct. We want to penalize if the absolute error is larger than 1, and if the mean squared error is somewhat bigger then machine epsilon.\n",
        "\n",
        "We have to execute the code now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cFBp-MkyYeoE"
      },
      "outputs": [],
      "source": [
        "np.finfo(np.float64).eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sNi129lYTpZ2"
      },
      "outputs": [],
      "source": [
        "def correctness_check(completions, **kwargs):\n",
        "    scores = []\n",
        "    # Generate some random matrices of size less than 128\n",
        "    A, A_list, B, B_list = generate_random_matrices(seed = np.random.randint(10000), n = 128)\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        if function is None or \"error\" in info:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            new_matmul = create_locked_down_function(function)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            pred = new_matmul(A_list.copy(), B_list.copy())\n",
        "        except:\n",
        "            # Failed!\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        true = np.matmul(A, B)\n",
        "        amax_error, mse_error = calculate_difference(pred, true)\n",
        "\n",
        "        # Check correctness and score!\n",
        "        machine_epsilon = 100*np.finfo(np.float64).eps\n",
        "        if   amax_error >= 3:   score = -3.0\n",
        "        elif amax_error >= 2:   score = -2.5\n",
        "        elif amax_error >= 1:   score = -2.0\n",
        "        elif amax_error >= 0.5: score = -1.0\n",
        "        elif amax_error >= 100*machine_epsilon: score = 0.0\n",
        "        elif amax_error >= machine_epsilon: score = 1.0\n",
        "        else: score = 3.0\n",
        "\n",
        "        if   mse_error >= 3:   score += -3.0\n",
        "        elif mse_error >= 2:   score += -2.5\n",
        "        elif mse_error >= 1:   score += -2.0\n",
        "        elif mse_error >= 0.5: score += -1.0\n",
        "        elif mse_error >= 100*machine_epsilon: score += 0.0\n",
        "        elif mse_error >= machine_epsilon: score += 1.0\n",
        "        else: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpTrofI9ZIn8"
      },
      "source": [
        "Finally our benchmarking function for `speed_check`! We shall limit the timer to 10 seconds and do 3 trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w5xkIAzuZMnO"
      },
      "outputs": [],
      "source": [
        "A, A_list, B, B_list = generate_random_matrices(seed = 0, n = 256)\n",
        "benchmarker = Benchmarker(trials = 3, timeout = 10)\n",
        "numpy_results = benchmarker.benchmark(np.matmul, [(A, B)])\n",
        "numpy_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mNDc6skFZZW6"
      },
      "outputs": [],
      "source": [
        "new_matmul = create_locked_down_function(extract_function(prompt))\n",
        "new_results = benchmarker.benchmark(new_matmul, [(A_list, B_list)])\n",
        "new_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noUAaX24aqNS"
      },
      "source": [
        "We can take the difference and do a negative sign for slower ones. If the ratio is less than 1 (ie faster, we shall invert it!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0IT9nXcjaI-X"
      },
      "outputs": [],
      "source": [
        "negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
        "positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
        "reward = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
        "reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ntYNFV0ra-MX"
      },
      "outputs": [],
      "source": [
        "new_results[\"median_ns\"] = 3\n",
        "numpy_results[\"median_ns\"] = 1000\n",
        "negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
        "positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
        "reward = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
        "reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HHmXmAxtbVpP"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def speed_check(completions, **kwargs):\n",
        "    scores = []\n",
        "    # Generate some random matrices of size less than 256\n",
        "    A, A_list, B, B_list = generate_random_matrices(seed = np.random.randint(10000), n = 256)\n",
        "    numpy_results = benchmarker.benchmark(np.matmul, [(A, B)])\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        function = extract_function(response)\n",
        "        if function is not None:\n",
        "            ok, info = check_only_stdlib_imports(function)\n",
        "        if function is None or \"error\" in info:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            new_matmul = create_locked_down_function(function)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        new_results = benchmarker.benchmark(new_matmul, [(A_list.copy(), B_list.copy())])\n",
        "\n",
        "        # Get score and clip to -10, 10\n",
        "        negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
        "        positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
        "        score = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
        "        if score >= 10:  score = 10\n",
        "        if score <= -10: score = -10\n",
        "        scores.append(score)\n",
        "    # Free memory to counteract OOMs\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCpSxtvSeAG_"
      },
      "source": [
        "We create the dataset which includes a replica of our prompt. Remember to add reasoning effort of low!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ldf6SjLHVPRv"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list([{\"prompt\" : [{\"role\": \"user\", \"content\": prompt.strip()}], \"answer\" : 0, \"reasoning_effort\": \"low\"}]*1000)\n",
        "maximum_length = len(tokenizer(prompt.strip())[\"input_ids\"])\n",
        "print(maximum_length)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations! We also support GSDP, GAPO, Dr GRPO and more! Go to our docs https://docs.unsloth.ai/ for more info!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-5,\n",
        "    weight_decay = 0.001,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 2, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 100,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        function_works,\n",
        "        no_cheating,\n",
        "        correctness_check,\n",
        "        speed_check,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQhtuwP4cf34"
      },
      "source": [
        "And let's train the model!\n",
        "\n",
        "**NOTE** A T4 free GPU might take 5 minutes for one generation sadly since it's an old GPU - A100 or H100 will be much faster!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VGRxPdSCcfC3"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "# Inference\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8BZZHOKiF9Ct"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": prompt}],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    reasoning_effort = \"low\",\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 1.0,\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 or MXFP4 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `mxfp4` for MXFP4 (OpenAI's GPT-OSS native precision). We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge and push to hub in mxfp4 4bit format\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"mxfp4\")\n",
        "if False: model.push_to_hub_merged(\"repo_id/repo_name\", tokenizer, token = \"hf...\", save_method = \"mxfp4\")\n",
        "\n",
        "# Merge and push to hub in 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V15Yhj1V9lwG"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}